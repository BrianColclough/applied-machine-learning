{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a488e41",
   "metadata": {},
   "source": [
    "# Assignment 2  \n",
    "**Applied Machine Learning**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. [20 pts]  \n",
    "At a high level (i.e., without entering into mathematical details), please describe, compare, and contrast the following classifiers:  \n",
    "\n",
    "- **Perceptron (textbook's version)**  \n",
    "    - A Perceptron is a type of artificial neural network and a foundational algorithms in machine learning, mainly used for binary classification. It takes input features, applies weights and a bias, and passes the weighted sum through an activation function to produce an output. During training, the Perceptron updates its weights whenever it makes errors, gradually improving its predictions through this adjustment process.\n",
    "- **SVM**  \n",
    "    - A supervised classifier that finds the maximum-margin decision boundary. With kernels it can capture non-linear structure. Speed is moderate and can be slow on large datasets, but strength and robustness are high with proper scaling/tuning. it uses numeric, scaled features. It’s a discriminative method that solves a convex optimization with hinge loss and regularization.\n",
    "- **Decision Tree**  \n",
    "    - A decision tree is a non-linear, rule-based classifier that splits the dataset into smaller subsets using criteria such as Gini impurity or information gain. Each split is made to maximize separation between the classes, and the model continues splitting until some stopping condition is met. Decision trees are easy to interpret and can handle both numerical and categorical features without requiring feature scaling. However, they are prone to overfitting and can be unstable, since small changes in the data may lead to very different tree structures. While they train quickly, a single tree can have high variance, making them less robust compared to ensemble methods.\n",
    "- **Random Forest** \n",
    "    - A random forest builds on the weaknesses of decision trees by combining many of them into an ensemble. It does this by training each tree on a random sample of the data and using a random subset of features at each split. The final prediction is made by aggregating the results of all trees through majority vote for classification. This approach reduces variance and overfitting, leading to more robust models. Random forests work well with mixed feature types, require little preprocessing, and are less sensitive to noise. The main drawbacks are that they can be slower to train and harder to interpret compared to a single decision tree.\n",
    "\n",
    "Some comparison criteria can be:  \n",
    "- Speed?  \n",
    "- Strength?  \n",
    "- Robustness?  \n",
    "- The feature type that the classifier naturally uses (e.g., relying on distance means that numerical features are naturally used)  \n",
    "- Is it statistical?  \n",
    "- Does the method solve an optimization problem? If yes, what is the cost function?  \n",
    "\n",
    "**Question:** Which one will be the first that you would try on your dataset?  \n",
    "\n",
    "- On typical tabular data, a Random Forest is what I would use for a first pass baseline since it's robust to noise/outliers, handles non-linearities and interactions, needs almost no scaling/encoding work. But depending on what the goal is and the type of data we are working with different classifiers might be needed. If interpretability is the top requirement, I’d add a shallow Decision Tree alongside the forest to produce a clear, human-readable explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc28053",
   "metadata": {},
   "source": [
    "### 2. [20 pts]  \n",
    "Define the following feature types and give example values from a dataset. You can pull examples from an existing dataset (like the Iris dataset) or you could write out a dataset yourself.  \n",
    "\n",
    "(Hint: In order to give examples for each feature type, you will probably have to use more than one dataset.)  \n",
    "\n",
    "- **Numerical**\n",
    "  - Quantitative values you can add/average; continuous or discrete.\n",
    "  *Examples:* Iris sepal length = 5.1 cm, petal width = 1.8 cm.\n",
    "\n",
    "- **Nominal**\n",
    "  - Categorical labels with **no natural order**.\n",
    "  *Examples:* Iris species (setosa, versicolor, virginica), color (red, blue, green).\n",
    "\n",
    "- **Date**\n",
    "  - Calendar/time stamps representing when an event occurred. Often expanded into features like year, month, day-of-week, hour.\n",
    "  *Examples:* order_date = 2025-09-03 and timestamp = 2024-07-15 13:05:22.\n",
    "\n",
    "- **Text**\n",
    "  - Unstructured strings of characters/tokens. Typically vectorized (e.g., bag-of-words, TF-IDF, embeddings).\n",
    "  *Examples:* review_text = “Loved the food, will return”\n",
    "\n",
    "- **Image**\n",
    "  - Pixel arrays (grayscale or RGB); high-dimensional signals.\n",
    "  *Examples:* MNIST digit image 28×28 (label “7”).\n",
    "\n",
    "- **Dependent variable**\n",
    "  - The target/response the model predicts (often denoted $y$).\n",
    "  *Examples:* Iris species label, house_price = $345,000, and “Chance of Admit” in the Graduate Admission dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c493f",
   "metadata": {},
   "source": [
    "\n",
    "### 3. [20 pts]  \n",
    "Using online resources, research and find other classifier performance metrics which are also as common as the accuracy metric. Provide the mathematical equations for them and explain in your own words the meaning of the different metrics you found.  \n",
    "\n",
    "Note that providing mathematical equations might involve defining some more fundamental terms. For example, you should define **“False Positive”** if you answer with a metric that builds on that.  \n",
    "\n",
    "- **True Positive (TP)**: predicted positive, and the actual class is positive.\n",
    "- **False Positive (FP)**: predicted positive, but the actual class is negative.\n",
    "- **False Negative (FN)**: predicted negative, but the actual class is positive.\n",
    "- **Specificity**\n",
    "     - of all the actual negatives, what fraction did the model correctly identify?\n",
    "    $$\n",
    "    \\mathrm{Specificity}= \\frac{TN+FP}{TN}\n",
    "    $$\n",
    "- **Precision**\n",
    "    - A metric that measures of the ones predicted to be true, how many were actually true. This measure is important for useages when false positives can be costly.\n",
    "    $$\n",
    "    \\mathrm{Precision} = \\frac{TP}{TP + FP}\n",
    "    $$\n",
    "- **Recall**\n",
    "    - Measues of all the true positives, what fraction did the model find. So this metric punishes false negatives. \n",
    "    $$\n",
    "    \\mathrm{Recall} = \\frac{TP}{TP + FN}\n",
    "    $$\n",
    "- **F1 Score**\n",
    "    - A single number that’s high only when both precision and recall are high. It penalizes an imbalance between them more than an arithmetic mean would.\n",
    "    $$\n",
    "    F_{1} = \\frac{2\\,\\mathrm{Precision}\\,\\mathrm{Recall}}\n",
    "                          {\\mathrm{Precision}+\\mathrm{Recall}}\n",
    "                     = \\frac{2TP}{2TP + FP + FN}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99bdd5",
   "metadata": {},
   "source": [
    "\n",
    "### 4. [40 pts]  Correlation Program From Scratch\n",
    "**Task:** Display the correlation matrix where each row and column are the features.  \n",
    "\n",
    "Additional questions to answer:  \n",
    "- Should we use *'Serial no'*? Why or why not?  \n",
    "    - We should **not** use Serial No. as this is a unique value that doesn't add any predictive signal to our model.\n",
    "- Observe that the diagonal of this matrix should have all 1's; why is this?  \n",
    "    - Because the correlation of any variable with itself is always going to be 1.\n",
    "- Since the last column can be used as the target (dependent) variable, what do you think about the correlations between all the variables?  \n",
    "    - GRE, TOEFL and GPA seem to be strongly correlated with each other, suggesting that we have some multicollinearity and overlapping information.\n",
    "- Which variable should be the most important to try to predict *'Chance of Admit'*\n",
    "    - CGPA has the highest correlation with Chance of Admit (~0.873), so it’s the single most predictive feature by correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881f1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "admission_data = pd.read_csv('Admission_Predict.csv')\n",
    "print(admission_data.shape)\n",
    "\n",
    "admission_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f432dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.802610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.835977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.791594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.711250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.675732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.669889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>0.873289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.802610</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.711250</td>\n",
       "      <td>0.675732</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.553202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
       "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
       "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
       "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
       "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
       "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
       "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
       "Research            0.580391     0.489858           0.447783  0.444029   \n",
       "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
       "\n",
       "                       LOR       CGPA  Research  Chance of Admit   \n",
       "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
       "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
       "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
       "SOP                0.729593  0.718144  0.444029          0.675732  \n",
       "LOR                1.000000  0.670211  0.396859          0.669889  \n",
       "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
       "Research           0.396859  0.521654  1.000000          0.553202  \n",
       "Chance of Admit    0.669889  0.873289  0.553202          1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#drop serial no\n",
    "admission_data_clean = admission_data.drop(columns=['Serial No.'])\n",
    "\n",
    "X = admission_data_clean.astype(float)\n",
    "\n",
    "def mean(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def stdev(values):\n",
    "    average = mean(values)\n",
    "    sample_variance = sum((value - average) ** 2 for value in values) / (len(values) - 1)\n",
    "    return math.sqrt(sample_variance)\n",
    "\n",
    "def covariance(x_values, y_values):\n",
    "    x_mean = mean(x_values)\n",
    "    y_mean = mean(y_values)\n",
    "\n",
    "    centered_products_sum = 0.0\n",
    "    for x_value, y_value in zip(x_values, y_values):\n",
    "        centered_x = x_value - x_mean\n",
    "        centered_y = y_value - y_mean\n",
    "        centered_products_sum += centered_x * centered_y\n",
    "\n",
    "    sample_covariance = centered_products_sum / (len(x_values) - 1)\n",
    "    return sample_covariance\n",
    "\n",
    "def correlation(x_values, y_values):\n",
    "    x_stdev = stdev(x_values)\n",
    "    y_stdev = stdev(y_values)\n",
    "    if x_stdev == 0 or y_stdev == 0:\n",
    "        return 0.0\n",
    "    return covariance(x_values, y_values) / (x_stdev * y_stdev)\n",
    "\n",
    "data = {feature_name: X[feature_name].tolist() for feature_name in X.columns}\n",
    "corr_matrix = [\n",
    "    [correlation(data[row_feature_name], data[col_feature_name]) for col_feature_name in X.columns]\n",
    "    for row_feature_name in X.columns\n",
    "]\n",
    "corr_df = pd.DataFrame(corr_matrix, index=X.columns, columns=X.columns)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b664fde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.802610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.835977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.791594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.711250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.675732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.669889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>0.873289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.802610</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.711250</td>\n",
       "      <td>0.675732</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.553202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
       "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
       "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
       "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
       "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
       "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
       "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
       "Research            0.580391     0.489858           0.447783  0.444029   \n",
       "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
       "\n",
       "                       LOR       CGPA  Research  Chance of Admit   \n",
       "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
       "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
       "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
       "SOP                0.729593  0.718144  0.444029          0.675732  \n",
       "LOR                1.000000  0.670211  0.396859          0.669889  \n",
       "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
       "Research           0.396859  0.521654  1.000000          0.553202  \n",
       "Chance of Admit    0.669889  0.873289  0.553202          1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output `.corr` matrix for comparison\n",
    "real_corr_matrix = admission_data_clean.corr()\n",
    "\n",
    "real_corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f25433",
   "metadata": {},
   "source": [
    "# References\n",
    "OpenAI. (2025). ChatGPT (GPT-5) [Large language model]. Retrieved from https://chat.openai.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
